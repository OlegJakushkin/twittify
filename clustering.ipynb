{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Кластеризация пользователей Twitter на основе текста твитов."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandasticsearch import DataFrame\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Импорт библиотек для обработки текстов."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import regex\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from emoji import UNICODE_EMOJI"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to ./data/...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path.append(os.path.abspath(os.getcwd() + '/data'))\n",
    "nltk.download('stopwords', download_dir='./data/')\n",
    "\n",
    "stem = Mystem()\n",
    "russian_stopwords = set(stopwords.words('russian'))\n",
    "russian_stopwords.update(['че', 'чё', 'мм', 'ммм', 'мммм', 'год',\n",
    "                          'кто', 'что', 'кого', 'чего', 'самый', 'самая',\n",
    "                          'аля', 'из', 'за', 'то', 'ой'])\n",
    "russian_stopwords.update(get_stop_words('russian'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Удаляем из текста стоп-слова, знаки препинания, слова с латинскими буквами и ссылки.\n",
    "Также приводим русские слова в нормальную форму."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def is_symbols(word):\n",
    "    return bool(regex.match(fr'[{punctuation} \\n_`«»“”️–]', word))\n",
    "\n",
    "def is_emoji(word):\n",
    "    return word in UNICODE_EMOJI\n",
    "\n",
    "def is_latin(word):\n",
    "    return bool(regex.match(r'\\p{IsLatin}', word))\n",
    "\n",
    "def is_number(word):\n",
    "    return bool(regex.match(r'\\d', word))\n",
    "\n",
    "def is_url(word):\n",
    "    return bool(regex.match(r'http[s]?://\\S+', word))\n",
    "\n",
    "def clear(sentence):\n",
    "    return ' '.join([w if w[0] != '@' and not is_url(w)\n",
    "                     else ''\n",
    "                     for w in sentence.split(' ') if len(w) > 0])\n",
    "\n",
    "def preprocess_tweets(tweets):\n",
    "    result = ''\n",
    "    for tweet in tweets:\n",
    "        text = []\n",
    "        for t in stem.lemmatize(clear(tweet).lower()):\n",
    "            token = t.strip()\n",
    "            if not token == '' \\\n",
    "            and not is_number(token) \\\n",
    "            and not is_emoji(token) \\\n",
    "            and not is_symbols(token) \\\n",
    "            and token not in russian_stopwords:\n",
    "                text.append(token)\n",
    "        result += ' '.join(text)\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Подключаемся к инстансу Elasticsearch, извлекаем из него данные и загружаем в датафрейм."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df = DataFrame \\\n",
    "    .from_es(url='http://localhost:9200', index='twittify-tweets', compat=7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Выбираем твиты на русском языке."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df = df.filter(df.language == 'ru') \\\n",
    "    .limit(10_000) \\\n",
    "    .select('nlikes', 'nreplies', 'nretweets', 'tweet', 'username', 'name') \\\n",
    "    .to_pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Игнорируем поля Elasticsearch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "df = df.drop(['_index', '_type', '_id', '_score', '_ignored'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Группируем твиты по пользователям, данные в численных столбцах усредняем, тексты твитов обрабатываем и объединяем."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df = df.groupby(df.username).aggregate(list)\n",
    "\n",
    "df.nlikes = df.nlikes.apply(np.mean)\n",
    "df.nreplies = df.nreplies.apply(np.mean)\n",
    "df.nretweets = df.nretweets.apply(np.mean)\n",
    "df.name = df.name.apply(lambda s: s[0])\n",
    "\n",
    "df.tweet = df.tweet.apply(preprocess_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Весь корпус текстов пропускаем через TF-IDF."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "corpus = df.tweet\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")\n",
    "tfidf.fit(corpus)\n",
    "text = tfidf.transform(corpus)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Кластеризуем алгоритмом K-Means.\n",
    "Добавляем метки кластера каждому пользователю."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "n_clusters = int(df.shape[0] / 40)\n",
    "clusters = KMeans(\n",
    "    n_clusters=n_clusters\n",
    ").fit(text)\n",
    "\n",
    "df['cluster'] = clusters.labels_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для поиска наиболее полезных ключевых слов группируем тексты твитов по кластерам.\n",
    "В итоге получится `k` документов, которые мы снова пропускаем через TF-IDF."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "corpus_cluster = df.groupby(df.cluster) \\\n",
    "    .aggregate(list) \\\n",
    "    .tweet \\\n",
    "    .apply(' '.join)\n",
    "\n",
    "tfidf_cluster = TfidfVectorizer(\n",
    "    min_df=5,\n",
    "    max_df=0.95\n",
    ")\n",
    "text_cluster = tfidf_cluster.fit_transform(corpus_cluster)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для каждого кластера выводим `n` наиболее часто встречающихся (ключевых) слов, которые используем для присваивания тэга."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0: начинать женщина ребенок вообще какой что думать делать понимать то\n",
      "Cluster #1: выборы задерживать минск обыск то власть путин суд дело навальный\n",
      "Cluster #2: уборка услуга лс интернет прием мегафон сайт магазин ярмарка заказ\n",
      "Cluster #3: брать игра какой понимать слово прочитывать вообще выкидывать кукла то\n",
      "Cluster #4: клуб цска мяч зенит тренер то гол игрок спартак матч\n",
      "Cluster #5: посылка техника москва почта подмосковье офисный номер отделение ремонт обслуживание\n",
      "Cluster #6: совет парк состояться посольство иностранный посол проходить камчатка российский федерация\n",
      "Cluster #7: аниме думать нравиться делать игра какой понимать что вообще то\n",
      "Cluster #8: уставать помнить либо спрашивать минута лапа вообще то понимать жрать\n",
      "Cluster #9: мойпервыйтвить продавать монета вложение конец запуск заработок купить зарабатывать биржа\n",
      "Cluster #10: понимать дерево санкт решать друг делать работать сделать мчс то\n",
      "Cluster #11: подписка код фильм мода оттенок скачать платформа электронный книга человечество\n",
      "Cluster #12: хор белый беларусь песня яна жыв бо ад па беларус\n",
      "Cluster #13: искусство соль заповедник поле военный история экскурсия парк выставка музей\n",
      "Cluster #14: президент миллиард путин закон власть никто проигрывать выборы сша новость\n",
      "Cluster #15: общество военно исторический советский война ссср ветеран вов история герой\n",
      "Cluster #16: то клиника зуб процедура путин космонавт профессор космос space мкс\n",
      "Cluster #17: победа молодежный мяч игра лига сборная тур игрок команда матч\n",
      "Cluster #18: министр программа участие сельский фонд принимать правительство губернатор развитие область\n",
      "Cluster #19: ой мудак концерт понимать то альбом ебать хуй нахуй блять\n",
      "Cluster #20: то москва заявлять вакцина президент страна российский власть украина путин\n",
      "Cluster #21: следующий ноябрь поколение семейство пк доступный консоль игра one xbox\n",
      "Cluster #22: делать парень понимать гавно витамин баба любить мужик то кулинарный\n",
      "Cluster #23: иллюстрация шоу iphone посмотреть tiktok фильм оружие импровизация калашников видео\n"
     ]
    }
   ],
   "source": [
    "for c, r in pd.DataFrame(text_cluster.todense()).iterrows():\n",
    "    frequent = [tfidf_cluster.get_feature_names()[i] for i in np.argsort(r)[-10:]]\n",
    "    print('Cluster #{}: {}'.format(c, ' '.join(frequent)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Предсказываем кластер произвольного пользователя и выводим несколько рекомендуемых пользователей."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: evapobeda20\n",
      "Cluster: 2\n"
     ]
    }
   ],
   "source": [
    "user = df.sample(n=1)\n",
    "tweet = tfidf.transform(user.tweet)\n",
    "cluster = clusters.predict(tweet)[0]\n",
    "\n",
    "print(f'User: {user.index[0]}')\n",
    "print(f'Cluster: {cluster}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: Издательство «Чёрная Сотня»\n",
      "Recommended: chernaya100book\n",
      "Full name: Пьяный Батя\n",
      "Recommended: pyanibatya\n",
      "Full name: xen bezdenezhnykh\n",
      "Recommended: saltyears_\n"
     ]
    }
   ],
   "source": [
    "recommended = df[df['cluster'] == cluster].sample(n=3)\n",
    "\n",
    "for i in range(recommended.shape[0]):\n",
    "    print(f'Full name: {recommended.name.iloc[i]}')\n",
    "    print(f'Recommended: {recommended.index[i]}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}